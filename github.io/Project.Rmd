---
title: "Practical Machine Learning Project"
author: "Jerry"
date: "September 22, 2015"
output: html_document
---
# Abstract

In this projects we the performance of 5 accelerometers and resultant class as data to create a model for predicting class.
We used a Random Forest to calculate the model based on a 60% subset of the training data.  This model was validate by
using the remaining 40% of the training data as a Validation set.  The model showed good accuracy, >98%.  When the model
was used to predict class for the 20 values in the test set it was correct 100% of the time.  

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

# Analysis

## Load required packages and set flags
```{r init - Load packages and set flags, message = FALSE, warning=FALSE}
require(data.table)
require(caret)
require(e1071)
require(randomForest)
require(parallel)
require(doParallel)

# set this to TRUE to reuse the model from the last run.
# This will greatly improves (i.e. removes 98% of the time) the speed the run.
USE_PREVIOUS_MODEL <- FALSE

```


## Load data files
Read in the files.  Convert excel div 0, nulls, and null string to NAs


```{r Load datafiles }
setInternet2(TRUE)

url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
origTraining <- fread(url, na.strings = c("NA","#DIV/0!", ""))

url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
origTest <- fread(url, na.strings = c("NA","#DIV/0!", ""))
```

## Data Cleaning and Filtering

We need to clean and filter data before we can use it.  As pointed out in the lectures this a 
critical step in the processing.

### Determine the predictors to use

We want to filter out data to only the parsimonious predictors that matter.
As specified in the instructions, we only want "belt, arm, forearm, or dumbell" columns.  And after
several tests we determine that we should use only the columns with data.  That is, no NAs.

```{r eliminate predictors with NA or "" values}
isAnyMissing <- sapply(origTest, function (x) any(is.na(x) | x == ""))
# we only care about data for the forearm, arms, dumbell, and belt and those that have values
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predictors <- names(isAnyMissing)[isPredictor]
```

### Filter Training dataset to Only Predictor and classe columns
We subset the columns in the data sets to only include the prediction columns from above and the "classe" column.  
Also, since classe has a discrete set of values we make the classe column a factor.
```{r subset training columns }
# use the classe + the predictor columns as a filter
colsToUse <- c("classe", predictors)
# Create filtered training set containing only the columns from the training file we chose above
filteredTraining <- origTraining[, colsToUse, with=FALSE]
# make classe a factor
filteredTraining <- filteredTraining[, classe := factor(filteredTraining[, classe])]
```

##  Split Filtered Training 60/40 into Training/Validation datasets

From the entire filtered training data we split the data into a training and a validation datasets
We will use the training dataset to train our model and the Validation dataset to validate the
model.

```{r split into training and test datasets}
# start date as seed.  Note the seed can cause small shift submitted answers
set.seed(160615)   
# partition the test data in to test and validation sets
inTrain <- createDataPartition(filteredTraining$classe, p=0.60, list=FALSE)
Train <- filteredTraining[inTrain,]
Validation <- filteredTraining[-inTrain,]
```

## Pre-process training dataset
```{r Preprocess training dataset}
trainPred <- Train[, predictors, with=FALSE]
# center and scale the data
preProc <- preProcess(trainPred, method=c("center", "scale"))
tempCentScale <- predict(preProc, trainPred)
trainCentScale <- data.table(data.frame(classe = Train[, classe], tempCentScale))
```

## Pre-process Validation dataset
```{r preprocess test dataset}
validationPred <- Validation[, predictors, with=FALSE]
# center and scale the data
preProc <- preProcess(validationPred, method=c("center", "scale"))
tempCentScale <- predict(preProc, validationPred)
validationCentScale <- data.table(data.frame(classe = Validation[, classe], tempCentScale))
```

## Check for constant predictors

Check to see if there are predictors with near 0 variance (constant) indicating 
they are un-informative.  We remove them since they would likely break the tree prediction or at least skew the model.
A quick review of the data shows that we should not have any of these.  If we do I want to know
about it and see if I made an error in my understanding of the data. 
See http://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/
```{r check near 0 variance}
nzvPredictors <- nearZeroVar(trainCentScale, saveMetrics=TRUE)
if (any(nzvPredictors$nzv)) {
  message(paste("ERROR - near zero values in Predictors.  Check these ", nzvPredictors$nzv)) 
} else  {
  message("No near 0 variance predictors.  Good to go!!")
}
```


## Train the model

Now that the data has been preprocessed we can train the model.  We are using the Random Forest model since the
values of classe are discrete, and as pointed out in the lectures, the Random Forest accuracy is high. 

Note: Also pointed out in the lectures was that the Random Forest can be a bit slow.  To combat this we use parallel clustering to take advantage of multiple processing cores available on many CPUs.  Even with this, it takes 24 minutes on a i7-4970K system with 16 GB of memory to train the model.
So there is also a facility to reuse the previously generated model.  This is useful while changing the report format.

```{r Train Model}

trainingFile <- "trainingModel.Rdata"
if ( !USE_PREVIOUS_MODEL ) {
  # If here, generate a new model
  
  # Enable parallel processing to make training faster
  require(parallel)
  require(doParallel)
  cl <- makeCluster(detectCores() - 1)
  registerDoParallel(cl)
  
  # Train the model
  system.time( trainingModel  <- train(classe ~ ., data=trainCentScale, method="rf"))
  # Terminate parallel processing
  stopCluster(cl)
  # Save the training model to disk
  save(trainingModel, file=trainingFile)
} else {
  #Restore training model from disk
  load(trainingFile)
}

```


## Evaluate the model on training set

Let's take a look at how the model compares to the training set.  Obviuosly we would expect high agreement
since this the data we trained the model against. If we do not have a high agreement the Random Forest may
not work with this data or we need more pre-processing.
```{r evaluate mode vs training set }
hatTrain <- predict(trainingModel, trainCentScale)
confusionMatrix(hatTrain, trainCentScale[, classe])
```
As the data shows we do indeed have high agreement between training data and the produced model.  Also note that "Reference" shows the classe
of the data has the following prevalence relation for number of each classe A > B > E > C > D.  However the counts are is relatively balanced so use of a random forest model should work.

The need for a balanced distribution was pointed out in the answer to this question, http://stat.stackexchange.com/questions/30691/how-to-interpret-oob-and-confusion-matrix-for-random-forest.  The 3rd answer
begins with " Your set is sharply unbalanced -- RF usually fails in this scenario (i.e. predicts well only the bigger class)"

## Evaluate the model on Validation set

Let's validate the model against our 40% of the training data we used to create a validation set.
We should see good accuracy, and and similar prevelence and balance amoung the classe entries.

```{r evaluate model vs validation set }
hatValidation <- predict(trainingModel, validationCentScale)
cmValidation <- confusionMatrix(hatValidation, validationCentScale[, classe])
cmValidation
```
We get an accuracy of `r cmValidation$overall['Accuracy']`.  Also, we see the the prevalence relationship
is still classe  A > B > E > C >D and the counts are relatively balanced, so this accuracy is likely real.
Checking the Sensitivity (True Postive Rate) and Specificity (True Negative Rate) for the classes we see these 
values are high so this helps confirm that the model does indeed correctly predict classe. 

## OOB errors
As a cross check lets take a look at the model's OOB estimate of error for our classe values shown in Figure 1.  We see 
that the OOB error rate is low, 0.89%, increasing our confidence in the model.

## Predict with test data 

We can now run the prediction with our model to obtain the answers need for the evaluation.
```{r Predict with test data}
testPred <- predict(preProc, origTest[, predictors, with=FALSE])
testHat <- predict(trainingModel, testPred)
```

### Create answer files
Using the provided function, we create the answer files which were submitted to Coursera.

```{r write answer files}
# Write submission answer files to send to Coursera using the provided function
pml_write_files = function(x){
  n = length(x)
  path <- "./answers"
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(testHat)
```

### Answer file correctness

The created answer files were accepted by the Coursera grader. 20 of 20 answers were correct.




# Appendix

## Model information

### Figure 1. Final Model Summary
``` {r echo=FALSE}
trainingModel$finalModel
```

